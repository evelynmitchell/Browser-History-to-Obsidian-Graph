{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d601130c",
   "metadata": {},
   "source": [
    "## Making Obsidian Graphs from Internet Browsing Sessions\n",
    "This project takes your chrome history and turns it into an obsidian graph. The graph shows your browsing sessions, which are smartly summarized, categorized, and tagged using GPT and DBSCAN. \n",
    "\n",
    "To use this notebook, you'll need to set a filepath to your chrome history and provide your OpenAI API key. Running this notebook costs about 15c per 500 web browsing sessions.\n",
    "\n",
    "Next Steps: \n",
    "- Better tagging & clustering systems\n",
    "- Time-based filtering\n",
    "- Bring in favorites/bookmarks?\n",
    "- revisit page-based data\n",
    "- more cleanup if people are interested in seeing this project develop (EG PCA is essentially unused in final output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "6b2661a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import openai\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "25d81515",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input these variables to get rollin'\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = ''\n",
    "\n",
    "# Chrome History File Location\n",
    "history_filepath = 'History'\n",
    "# Typically located at:\n",
    "# Windows: C:\\Users\\<username>\\AppData\\Local\\Google\\Chrome\\User Data\\Default\n",
    "# Mac: /Users/<username>/Library/Application Support/Google/Chrome/Default\n",
    "# Linux: /home/<username>/.config/google-chrome/Default\n",
    "\n",
    "# Obsidian vault filepath\n",
    "obsidian_vault = 'obsidian_vault'\n",
    "\n",
    "# Are you running this notebook for the first time, and generating summaries, embeddings, and tags?\n",
    "# otherwise this notebook will try to load this data.\n",
    "generate = True\n",
    "\n",
    "# File name to save & later load your GPT generated browsing session descriptions\n",
    "session_summaries = 'summaries.csv'\n",
    "\n",
    "# File name to save & later load GPT generated embeddings\n",
    "session_embeddings = 'embeddings.csv'\n",
    "\n",
    "# File name for our assembled data\n",
    "assembled_data = 'assembled_data.csv'\n",
    "\n",
    "### More options\n",
    "session_timeout = pd.Timedelta(minutes=30) # Define a session timeout threshold (e.g., 30 minutes)\n",
    "# DBSCAN parameters, which determine our clustering\n",
    "EPS = 40\n",
    "MIN_SAMPLES = 2\n",
    "FULL_EMBEDDINGS = True # True means DBSCAN will use all of our embedding dimensions for clustering, otherwise PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c18be1",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25abd5e",
   "metadata": {},
   "source": [
    "## Loading Data from your Chrome History file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca0a0734",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(history_filepath) # Opening Connection\n",
    "\n",
    "# Reading Data\n",
    "query = \"\"\"\n",
    "SELECT * \n",
    "FROM urls, visits\n",
    "WHERE urls.id = visits.url\n",
    "ORDER BY visits.visit_time DESC\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "df.columns.values[8] = 'visit_id' # Rename the additional 'url' column\n",
    "df['visit_time'] = pd.to_datetime((df['visit_time'] - 11644473600000000) // 1000000, unit='s') # create a readable timestamp\n",
    "df = df.sort_values(by='visit_time')\n",
    "conn.close() # Closing Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ca67bf",
   "metadata": {},
   "source": [
    "## Parsing URL into Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b89a346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subdomain(domain):\n",
    "    parts = domain.split('.')\n",
    "    if len(parts) > 2:\n",
    "        return parts[0]\n",
    "    return ''\n",
    "\n",
    "def extract_file_extension(path):\n",
    "    if '.' in path:\n",
    "        return path.split('.')[-1]\n",
    "    return ''\n",
    "\n",
    "def extract_language_indicator(path):\n",
    "    # Common language codes (extend this list as needed)\n",
    "    lang_codes = ['en', 'es', 'fr', 'de', 'zh', 'ru', 'ja', 'pt', 'it']\n",
    "    pattern = r'\\/(' + '|'.join(lang_codes) + r')\\/'\n",
    "    match = re.search(pattern, path)\n",
    "    return match.group(1) if match else ''\n",
    "\n",
    "def parse_url(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc\n",
    "    path = parsed_url.path\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "\n",
    "    subdomain = extract_subdomain(domain)\n",
    "    file_extension = extract_file_extension(path)\n",
    "    language_indicator = extract_language_indicator(path)\n",
    "\n",
    "    utms = {key: query_params[key][0] for key in query_params if key.startswith('utm_')}\n",
    "    \n",
    "    return domain, path, query_params, utms, subdomain, file_extension, language_indicator\n",
    "\n",
    "# Apply the parsing function to the renamed URL column\n",
    "df[['domain', 'path', 'query_params', 'utms', 'subdomain', 'file_extension', 'language_indicator']] = df['url'].apply(lambda x: pd.Series(parse_url(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d447053f",
   "metadata": {},
   "source": [
    "## Extracting Session Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1a012f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "543"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_ids = [] # Initialize a list to store session ids\n",
    "session_id = 0\n",
    "\n",
    "# Iterate through the DataFrame and assign session ids\n",
    "for i in range(1, len(df)):\n",
    "    if df.iloc[i]['visit_time'] - df.iloc[i-1]['visit_time'] > session_timeout:\n",
    "        session_id += 1\n",
    "    session_ids.append(session_id)\n",
    "    \n",
    "# The first row is the start of the first session\n",
    "session_ids.insert(0, 0)\n",
    "\n",
    "# Add the session ids to the DataFrame\n",
    "df['session_id'] = session_ids\n",
    "\n",
    "df['session_id'].nunique() # peeking number of unique sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3493ea",
   "metadata": {},
   "source": [
    "## Summarizing Sessions Based on Page Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "923f3c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates the text data we will pass to openai - it takes page titles in the session,\n",
    "# limits them to 127 characters, and then joins them together\n",
    "session_texts = df.groupby('session_id')['title'].apply(\n",
    "    lambda titles: ' '.join(set(title[:127] + '|' for title in titles))\n",
    ")\n",
    "# session_texts # Uncomment to preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c0625952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our summarization function using GPT 3.5 Turbo\n",
    "def summarize_text(text, model=\"gpt-3.5-turbo\"):\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that comes up with succinct summaries for web browsing sessions. You don't say things like 'exploring' or other self-evident verbs.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Come up with a concise title with no fluff or wasted wordcount that summarizes a browsing session with the following page titles: \\n\\n\" + text}\n",
    "            ],\n",
    "            max_tokens=100,  # Adjust as needed\n",
    "            temperature=0.7\n",
    "        )\n",
    "        summary = response['choices'][0]['message']['content'].strip()\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "    \n",
    "# # Code for tuning the above Summarization function\n",
    "# document = session_texts[100][:1024]\n",
    "# summary = summarize_text(document)\n",
    "# print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ac7b0dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPENDS MONEY ### Applying session summarization. Also takes time. Approximately 7 cents & 3 mins per 500 sessions\n",
    "if generate == True:\n",
    "    # Apply the summarize_text function to each text in session_texts\n",
    "    summaries = [summarize_text(text[:1024]) for text in session_texts]\n",
    "else:\n",
    "    summaries = pd.read_csv(session_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0d56d23c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converting our summaries to a dataframe\n",
    "summaries_df = pd.DataFrame(summaries, columns=['Summaries'])\n",
    "\n",
    "# Saving our summaries\n",
    "if generate == True:\n",
    "    summaries_df.to_csv(session_summaries, index=False)\n",
    "    \n",
    "# Creating a dictionary mapping the DataFrame's index to summaries\n",
    "summary_dict = summaries_df['Summaries'].to_dict()\n",
    "\n",
    "# Map our summaries to the DataFrame using the index\n",
    "df['Summaries'] = df['session_id'].map(summary_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f759d32",
   "metadata": {},
   "source": [
    "### Data Cleaning - Identifying top domains, truncated versions of text fields, and visit time formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "615b7511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill any NaNs in title\n",
    "df['title'].fillna('No Page Title', inplace=True)\n",
    "\n",
    "# Ensure 'visit_time' is in datetime format\n",
    "df['visit_time'] = pd.to_datetime(df['visit_time']) \n",
    "# Convert 'visit_time' to string for better display in hover data\n",
    "df['visit_time_str'] = df['visit_time'].astype(str) \n",
    "# Convert 'visit_time' to a numerical format (e.g., seconds since the start of the dataset)\n",
    "df['visit_time_numeric'] = (df['visit_time'] - df['visit_time'].min()).dt.total_seconds() \n",
    "\n",
    "# Find the top 15 most-visited domains (useful for visualizations)\n",
    "top_domains = df['domain'].value_counts().nlargest(15).index \n",
    "# Add most-visited domains to a column in our dataframe\n",
    "df['top_domain'] = df['domain'].apply(lambda x: x if x in top_domains else 'Other')\n",
    "\n",
    "# Truncated versions of 'url', 'title', & 'summary' for labels\n",
    "df['url_truncated'] = df['url'].str[:80]\n",
    "df['title_truncated'] = df['title'].str[:80]\n",
    "df['summary_truncated'] = df['Summaries'].str[:128]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a46c91a",
   "metadata": {},
   "source": [
    "## Generating Document Embeddings for Categorization & Mapping\n",
    "Document Embeddings give us the ability to understand the semantic space of our earlier summarizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fb2415db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating embeddings for semantic similarity of our document summaries\n",
    "def generate_embeddings(text, model=\"text-embedding-3-small\"):\n",
    "    try:\n",
    "        response = openai.Embedding.create(\n",
    "            model=model,\n",
    "            input=text\n",
    "        )\n",
    "        embedding = response['data'][0]['embedding']\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2163b02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(543, 1536)\n"
     ]
    }
   ],
   "source": [
    "# Get embeddings that describe our sessions - if generating, this takes time & some money (usually less than a penny)\n",
    "if generate:\n",
    "    # Generate embeddings from summaries and convert to DataFrame, then save them\n",
    "    embeddings = [generate_embeddings(summary) for summary in summaries]\n",
    "    embeddings_df = pd.DataFrame(embeddings)\n",
    "    embeddings_df.to_csv(session_embeddings, index=False) # Save the DataFrame to CSV\n",
    "else:\n",
    "    # Load a saved embeddings DataFrame\n",
    "    embeddings_df = pd.read_csv(session_embeddings)\n",
    "\n",
    "# Creating an array from our saved embeddings DataFrame\n",
    "embeddings_array = embeddings_df.to_numpy()\n",
    "print(embeddings_array.shape)  # Display the shape of our embeddings array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13514d3b",
   "metadata": {},
   "source": [
    "## reduce our features to fewer dimensions using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2799b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some due-diligence with embedding quality\n",
    "# Filter out None values from embeddings\n",
    "filtered_embeddings = [embedding for embedding in embeddings if embedding is not None]\n",
    "\n",
    "# Find the maximum length among the non-None embeddings\n",
    "max_length = max(len(embedding) for embedding in filtered_embeddings)\n",
    "\n",
    "# Pad embeddings with zeros\n",
    "padded_embeddings = [np.pad(embedding, (0, max_length - len(embedding)), 'constant') if embedding is not None else np.zeros(max_length) for embedding in embeddings]\n",
    "\n",
    "# Convert the list of embeddings to a NumPy array\n",
    "embeddings_array = np.array(padded_embeddings)\n",
    "\n",
    "# Check if all embeddings are of the same length\n",
    "if not all(len(embedding) == len(padded_embeddings[0]) for embedding in padded_embeddings):\n",
    "    raise ValueError(\"Not all embeddings are of the same length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "55162dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PCA\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# Fit and transform the data to reduce it to fewer dimensions\n",
    "PCA_embeddings = pca.fit_transform(embeddings_array)\n",
    "\n",
    "# Creating a df to hold our PCA results\n",
    "PCA_df = pd.DataFrame(data=PCA_embeddings, columns=['PCA_X', 'PCA_Y', 'PCA_Z'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda1428",
   "metadata": {},
   "source": [
    "### Creating a Session-based dataframe & adding our Principle Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "217be1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DataFrame size: (543, 12)\n",
      "Session_id values: 543\n"
     ]
    }
   ],
   "source": [
    "# Creating session_df based on 'session_id'\n",
    "session_df = df.groupby('session_id').agg({\n",
    "    'Summaries': 'first',\n",
    "    'visit_time_str': lambda x: list(x),       # Collect all visit times in a list\n",
    "    'title': lambda x: list(x),                # Collect all titles in a list\n",
    "    'url': lambda x: list(x),                  # Collect all URLs in a list\n",
    "    'domain': lambda x: list(x),                  # Collect all URLs in a list\n",
    "    'visit_count': lambda x: list(x),          # Collect all visit counts in a list\n",
    "    'typed_count': lambda x: list(x),          # Collect all typed counts in a list\n",
    "    'top_domain': lambda x: list(x)      # Collect all domain categories in a list\n",
    "}).reset_index()\n",
    "\n",
    "# Merge PCA with session_df\n",
    "PCA_df['session_id'] = sorted(pd.unique(session_df['session_id']))\n",
    "session_df = session_df.merge(PCA_df, on='session_id', how='left')\n",
    "\n",
    "# Final checks that no rows are being dropped\n",
    "print(\"Final DataFrame size:\", session_df.shape)\n",
    "print(\"Session_id values:\", len(session_df['session_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "8c1a1c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data\n",
    "if generate == True: \n",
    "    session_df.to_csv(assembled_data, index=False)\n",
    "else: # Import prepared data\n",
    "    session_df = pd.read_csv(assembled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbb9089",
   "metadata": {},
   "source": [
    "# Now that data is prepared..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c4f7f464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(543, 12)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# session_df # Visually check our prepared data\n",
    "\n",
    "# Check the shape to verify the structure\n",
    "session_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a960bcc",
   "metadata": {},
   "source": [
    "# Advanced Semantic Mapping\n",
    "Next we'll use our embeddings to identify categories and themes in our sessions, and then export our categorized data into Obsidian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb87ae",
   "metadata": {},
   "source": [
    "### Appending embeddings to our session data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6566fa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the embeddings array as a new column in the DataFrame\n",
    "session_df['Document_Embeddings'] = list(embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "48850a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the addition by checking the new DataFrame structure\n",
    "len(session_df['Document_Embeddings'][0])\n",
    "print(session_df.shape)\n",
    "session_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a078f9",
   "metadata": {},
   "source": [
    "### Identifying clusters in embeddings using DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "b9a46748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to overwrite variables for tuning\n",
    "# EPS = 40\n",
    "# MIN_SAMPLES = 2\n",
    "# FULL_EMBEDDINGS == True\n",
    "\n",
    "# Create a StandardScaler object for Zscore Normalization\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Option to choose the type of embeddings: 'full = True' for Document_Embeddings, else just use PCA components\n",
    "if FULL_EMBEDDINGS == True:\n",
    "    # Assuming 'Document_Embeddings' is already in the correct format (list of arrays)\n",
    "    DBSCAN_IN = np.array(session_df['Document_Embeddings'].tolist())\n",
    "else:\n",
    "    # Stack the PCA components into a single numpy array\n",
    "    DBSCAN_IN = session_df[['Embedding_X', 'Embedding_Y', 'Embedding_Z']].values\n",
    "\n",
    "# Normalize our input for DBSCAN\n",
    "DBSCAN_IN = scaler.fit_transform(DBSCAN_IN)    \n",
    "\n",
    "# Apply DBSCAN\n",
    "# Note: Adjust eps and min_samples based on the scale and distribution of your data\n",
    "dbscan = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES)  # These parameters may need to be tuned\n",
    "session_df['Cluster'] = dbscan.fit_predict(DBSCAN_IN)\n",
    "\n",
    "# Filter out noise (-1 labels are considered noise in DBSCAN)\n",
    "filtered_data = session_df[session_df['Cluster'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "3c7f5b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(223, 14)\n"
     ]
    }
   ],
   "source": [
    "# Code for tuning clustering parameters (EPS & MIN_SAMPLES)\n",
    "# print(filtered_data.shape)\n",
    "# display(filtered_data['Cluster'].value_counts())\n",
    "# filtered_data # showing the rows that do have identified categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075a4f90",
   "metadata": {},
   "source": [
    "### Create descriptor tags for our clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "0da8aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tags(texts):\n",
    "    # Ensure texts is a single string as the prompt expects a single coherent string input\n",
    "    prompt = \"Generate a list of 10 single-word tags that best describe the underlying conceptual categories the following descriptions all have in common: \\n\\n\" + \"\\n\".join(texts)\n",
    "    \n",
    "    # Using chat completions endpoint\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that operates as a concise category identifier.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=150,\n",
    "        temperature=0.7,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "5661fe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "### COSTS MONEY ###\n",
    "if generate == True:\n",
    "    # Dictionary to hold descriptions for each cluster\n",
    "    cluster_descriptions = {}\n",
    "    \n",
    "    # Generate descriptions for each cluster and store them in the dictionary\n",
    "    for cluster_id in filtered_data['Cluster'].unique():\n",
    "        cluster_summaries = filtered_data[filtered_data['Cluster'] == cluster_id]['Summaries'].unique()[:15]\n",
    "        if len(cluster_summaries) > 0:\n",
    "            description = generate_tags(cluster_summaries)\n",
    "            cluster_descriptions[cluster_id] = description\n",
    "        else:\n",
    "            cluster_descriptions[cluster_id] = \"No description available\"\n",
    "        \n",
    "    # Map the descriptions back to the session_df based on Cluster IDs\n",
    "    session_df['Cluster_Name'] = session_df['Cluster'].map(cluster_descriptions)\n",
    "\n",
    "    # Write updated session_df to our file\n",
    "    session_df.to_csv(assembled_data, index=False)    \n",
    "\n",
    "else:\n",
    "    session_df = pd.read_csv(assembled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "666b20e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking our results\n",
    "# session_df[session_df['Cluster'] == 1].head(1)\n",
    "# session_df['Cluster_Name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7446c4",
   "metadata": {},
   "source": [
    "### Creating an Obsidian Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ee22dd",
   "metadata": {},
   "source": [
    "### Some data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "3ebeffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert date strings from a specific pattern to 'mm/dd/yy' format\n",
    "def convert_dates(date_str):\n",
    "    date_str = str(date_str)  # Convert to string to ensure compatibility\n",
    "    date_pattern = r\"Timestamp\\('(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})'\\)\"\n",
    "    dates = re.findall(date_pattern, date_str)\n",
    "    \n",
    "    if dates:\n",
    "        first_date = datetime.strptime(dates[0], \"%Y-%m-%d %H:%M:%S\")\n",
    "        return first_date.strftime(\"%m/%d/%y\")\n",
    "    return None\n",
    "\n",
    "# Ensure there are no NaN values in the column to avoid type errors\n",
    "session_df['visit_time_str'] = session_df['visit_time_str'].fillna('Unknown')\n",
    "\n",
    "# Apply the conversion function to the 'visit_time_str' column\n",
    "session_df['Simple_Date'] = session_df['visit_time_str'].apply(convert_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "453e9ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the result\n",
    "# print(session_df[['visit_time_str', 'Simple_Date']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e281ab6a",
   "metadata": {},
   "source": [
    "### Creating our Obsidian Vault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "d7160320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      None\n",
       "1      None\n",
       "2      None\n",
       "3      None\n",
       "4      None\n",
       "       ... \n",
       "538    None\n",
       "539    None\n",
       "540    None\n",
       "541    None\n",
       "542    None\n",
       "Length: 543, dtype: object"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = obsidian_vault\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def sanitize_filename(text):\n",
    "    sanitized = \"\".join([c if c.isalnum() or c in [' ', '-', '_'] else ' ' for c in text]).strip()\n",
    "    return ' '.join(sanitized.split()[:20])[:150]\n",
    "\n",
    "def extract_tags(cluster_name):\n",
    "    tags = re.split(r'\\d+\\.\\s*', cluster_name)\n",
    "    return [tag.replace(' ', '_') for tag in tags if tag.strip()]\n",
    "\n",
    "def create_markdown(row):\n",
    "    filename = f\"{sanitize_filename(row['Summaries'])}.md\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    # Normalize domain data, now just check if the list is empty or None\n",
    "    domains = set(row['domain']) if row['domain'] else set()\n",
    "    linked_domains = [f\"[[{domain}]]\" for domain in domains if domain.strip()]\n",
    "\n",
    "    # Handling URLs which may already be a list or a string\n",
    "    urls = row['url'] if isinstance(row['url'], list) else row['url'].split(',')\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        file.write(\"---\\n\")\n",
    "        file.write(f\"session_id: {row['session_id']}\\n\")\n",
    "        file.write(\"tags:\\n\")\n",
    "        tags = extract_tags(row['Cluster_Name']) if pd.notna(row['Cluster_Name']) else [\"No_Category\"]\n",
    "        for tag in tags:\n",
    "            file.write(f\"  - {tag}\\n\")\n",
    "        file.write(\"---\\n\\n\")\n",
    "        file.write(f\"# {row['Summaries']}\\n\\n\")\n",
    "        file.write(\"**Visited Domains**:\\n\" + ' '.join(linked_domains) + \"\\n\\n\")\n",
    "        file.write(\"<details>\\n<summary>History Details</summary>\\n\")\n",
    "        file.write(\"- **Full URLs**:\\n\" + '\\n'.join([f\"- {url}\" for url in urls]) + \"\\n\")\n",
    "        file.write(f\"- **Visited on**: {row['visit_time_str']}\\n\")\n",
    "        file.write(\"</details>\\n\")\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "session_df.apply(create_markdown, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a8e590",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743d904b",
   "metadata": {},
   "source": [
    "This was a fun project to learn more about Obsidian vaults & storage sytems, Chrome history file formats, DBSCAN, and working with OpenAI APIs to move up orders of abstraction to summarize and tag things. Navigating browser history & favorites folders is always a pain so it's fun to see some other possibilities, and while I think the result here is interesting I think in it's current state it's even less navigable & requires some more development (such as more time filtering, smarter tagging, and some other tricks) to become a useful tool.\n",
    "\n",
    "I'm not sure if Obsidian is actually flexible enough a foundation for the features that this project may need, so I'll investigate what sort of plugin flexibility it has."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac8ff5b",
   "metadata": {},
   "source": [
    "# Scrap Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e25551a",
   "metadata": {},
   "source": [
    "### Hashing Vectorizer\n",
    "Hashing Vectorizer to PCA is fast & free but doesn't have semantic understanding & therefore has worse clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c0e3beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# # Define the number of features (dimensions) you want after hashing\n",
    "# n_features = 6  # You can adjust this\n",
    "\n",
    "# # apply hashing vectorizer to our session_summaries\n",
    "# hasher = HashingVectorizer(n_features=n_features, alternate_sign=False)\n",
    "# hashed_features = hasher.transform(df['session_summary'])\n",
    "\n",
    "# # Convert to a dense array if it's not already\n",
    "# hashed_features_dense = hashed_features.toarray()\n",
    "\n",
    "# # Convert the dense array to a DataFrame\n",
    "# hashed_df = pd.DataFrame(hashed_features_dense)\n",
    "\n",
    "# # Optionally, prefix the column names to indicate they are hashed features\n",
    "# hashed_df.columns = ['hash_feature_' + str(i) for i in range(hashed_df.shape[1])]\n",
    "\n",
    "# # Reset index of your original DataFrame if necessary\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Concatenate the DataFrames\n",
    "# df_hash = pd.concat([df, hashed_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "25dab890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_hash.head(2) # preview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
